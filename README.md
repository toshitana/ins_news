# 生命保険会社ニュースリリースチェッカー

このプロジェクトは、指定された日本の生命保険会社のウェブサイトを巡回し、直近N日間の新しいニュースリリースを検索してCSVファイルに出力することを目的としたスクリプト `news_scraper.py` を含みます。

## 実行方法

1. `companies.csv` に対象の会社名とニュースリリースページのURLを記述します。
2. 必要であれば、`news_scraper.py` 内の `days` 変数を変更して検索対象の日数を調整します（デフォルトは7日）。
3. 以下のコマンドを実行します。
   ```bash
   pip install requests beautifulsoup4
   python news_scraper.py
   ```
   スクリプトは、発見されたニュースリリースを `YYYY_MM_DD-YYYY_MM_DD.csv` という名前のファイルに出力します。該当ニュースがない場合は、ヘッダーのみのCSVファイルが作成されるか、ファイルが作成されないことがあります（現在のスクリプトの挙動による）。

## 現在の状況と課題

### ニューススクレイピング (`news_scraper.py`)

- スクリプトは指定された各社サイトへのアクセスを試みますが、いくつかのサイトではエラーにより情報を取得できていません。
- **スキップされたサイト**: `skipped_sites.log` に記載の通り、403 Forbidden, Timeout, SSL Error, Content Not Found (JavaScriptレンダリングが必要な可能性) などの理由で一部サイトは処理されていません。
- **HTML解析の課題**: 現在の `news_scraper.py` は、非常に汎用的なHTMLセレクタを使用しています。しかし、各保険会社のウェブサイトのHTML構造は大きく異なるため、このアプローチでは多くのサイトでニュースリリースを正確に特定・抽出することができません。このため、現状ではニュースの発見には至っていません。
- **CSV出力**: ニュースが見つからない場合、`news_scraper.py` は結果のCSVファイルを作成しますが、中身はヘッダーのみとなります。

### セレクタ情報 (`site_selectors.csv`)

各社のニュースリリースページからニュース記事のタイトル、日付、URLを特定するためのCSSセレクタ（またはXPath）を収集し、`site_selectors.csv` にまとめました。このファイルは、`news_scraper.py` のHTML解析ロジックを改善するための基礎情報となります。

- **ファイル形式**: CSV形式
- **カラム**: `会社名`, `ニュースリスト親要素セレクタ`, `記事タイトルセレクタ`, `日付セレクタ`, `記事リンクセレクタ`
- **特記事項**:
    - 一部のサイトでは、日付情報が特定のHTMLタグに囲まれておらず、親要素のテキストノードから正規表現で抽出する必要がありました。これらは日付セレクタカラムに `REGEX_DATE:` というプレフィックスで示されています（例: `REGEX_DATE:\d{4}/\d{2}/\d{2}`）。
    - アクセスできなかったサイトや、コンテンツが取得できなかったサイト（主にJavaScriptレンダリングが必要と思われるページ）については、セレクタ情報に `SKIPPED_...` と記録されています。

## 今後の改善点

1.  **`news_scraper.py` のHTML解析ロジックの強化**:
    *   `site_selectors.csv` に記載された各社固有のセレクタ情報を読み込み、それに基づいてニュース項目を抽出するように `get_news_releases` 関数を大幅に改修する。
    *   `REGEX_DATE:` で指定された日付セレクタに対応するため、正規表現による日付抽出ロジックを組み込む。
    *   相対URLを絶対URLに正しく変換する処理の堅牢性を高める。
2.  **エラーハンドリングとロギングの改善**:
    *   `skipped_sites.log` に加えて、各サイトの処理状況（成功、失敗、スキップ理由など）をより詳細に記録する。
    *   タイムアウトエラーについては、リトライ処理の導入を検討する。
3.  **動的コンテンツへの対応 (高度な課題)**:
    *   JavaScriptレンダリングが必要なサイト（`SKIPPED_BLANK_PAGE` や `SKIPPED_CONTENT_NOT_FOUND` となったサイト）に対応するためには、SeleniumやPlaywrightのようなブラウザ自動化ツールの導入を検討する。これは今回のタスク範囲を超える可能性があります。
4.  **CSV出力の改善**:
    *   現状、ニュースがない場合はヘッダーのみのCSVが出力されますが、実行結果（何件中何件成功、スキップ件数など）をまとめたサマリー情報も出力するか、ログで明確にすることが望ましい。
5.  **設定の外部化**:
    *   検索日数などのパラメータをコマンドライン引数や設定ファイルから読み込めるようにする。

このプロジェクトは、各社ウェブサイトの構造に大きく依存するため、継続的なメンテナンスとセレクタ情報の更新が重要となります。
