# 生命保険会社ニュースリリースチェッカー

このプロジェクトは、指定された日本の生命保険会社のウェブサイトを巡回し、直近N日間の新しいニュースリリースを検索してCSVファイルに出力することを目的としたスクリプト `news_scraper.py` を含みます。

## 実行方法

1. `companies.csv` に対象の会社名とニュースリリースページのURLを記述します。
2. 必要であれば、`news_scraper.py` 内の `days` 変数を変更して検索対象の日数を調整します（デフォルトは7日）。
3. 以下のコマンドを実行します。
   ```bash
   pip install requests beautifulsoup4
   python news_scraper.py
   ```
   スクリプトは、発見されたニュースリリースを `YYYY_MM_DD-YYYY_MM_DD.csv` という名前のファイルに出力します。該当ニュースがない場合は、ヘッダーのみのCSVファイルが作成されるか、ファイルが作成されないことがあります（現在のスクリプトの挙動による）。

## 現在の状況と課題

### ニューススクレイピング (`news_scraper.py`)

- **目的**: `companies.csv` に記載された各生命保険会社のウェブサイトから、指定された期間内の新しいニュースリリースを抽出し、CSVファイルに出力する。
- **セレクタ情報の利用**: `site_selectors.csv` に記載された各社固有のCSSセレクタ情報を利用して、ニュース記事のリスト、タイトル、日付、URLの抽出を試みる。
- **現状の動作**:
    - `site_selectors.csv` の読み込み、基本的なセレクタ適用ロジック、強化された日付パース処理、デバッグログ出力機能は実装済み。
    - しかし、多くのサイトで `site_selectors.csv` に記載したリストセレクタがHTML構造と完全に一致せず、「No items found with list selector」という結果になり、ニュース記事の抽出には至っていない。
- **スキップされたサイト**: `skipped_sites.log` に記録されている通り、403 Forbidden, Timeout, SSL Error, Content Not Found (JavaScriptレンダリングが必要な可能性が高い) などの理由で一部サイトは処理対象外となっている。
- **CSV出力**: ニュースが見つからない場合、`news_scraper.py` は結果のCSVファイル（`YYYY_MM_DD-YYYY_MM_DD.csv`）を作成するが、中身はヘッダーのみとなる。

### セレクタ情報 (`site_selectors.csv`)

- 各社のニュースリリースページからニュース記事のタイトル、日付、URLを特定するためのCSSセレクタ情報を収集し、`site_selectors.csv` にまとめている。
- **課題**: `view_text_website` ツールの出力のみに依存してセレクタを特定したため、実際のブラウザレンダリング結果とHTML構造が異なる場合があり、セレクタの精度が低い。これがニュース抽出失敗の最大の原因。

## 今後の改善点と取組方針

現行の `view_text_website` ツールのみでは、特に動的コンテンツを多用する現代のウェブサイトの正確なHTML構造解析とセレクタ特定には限界があることが判明しています。しかし、プロジェクトの重要性に鑑み、以下のアプローチで引き続きニュース抽出の成功に向けて最大限努力します。

1.  **セレクタの精密な再検証と段階的修正 (最優先)**:
    *   「No items found with list selector」エラーが出ているサイトを中心に、`view_text_website` の出力を再度徹底的に分析する。
    *   HTMLの階層構造、属性（classやidだけでなくdata属性なども考慮）、テキスト内容のパターンなどを手掛かりに、より堅牢なセレクタを推定し、`site_selectors.csv` を更新する。
    *   一社ずつ、または数社ずつ修正とテスト実行を繰り返し、少しでも抽出できるサイトを増やす。
2.  **`news_scraper.py` の抽出ロジックの柔軟性向上**:
    *   現在のセレクタが完全一致を前提としている箇所について、部分一致や代替セレクタの試行など、より柔軟な検索ロジックの導入を検討する（例：`*` ワイルドカードの活用、複数の候補セレクタを試すなど）。
    *   `REGEX_DATE:` のような特殊な指示子については、その意図通りに日付抽出が機能しているか、個別に検証・調整を行う。
3.  **フォールバック戦略の検討**:
    *   特定のセレクタで情報が取得できない場合に、より汎用的なタグ（例: `<article>`, `<item>`, `<td>` 内の日付らしき文字列など）やキーワード（例: 「PDF」, 「お知らせ」）を手がかりに情報を推測するフォールバックロジックを限定的に導入することを検討する。ただし、誤抽出のリスクとのバランスを考慮する。
4.  **詳細なエラー分析とログ**:
    *   どのセレクタが失敗したかだけでなく、そのセレクタで何が取得できたか（あるいはできなかったか）の断片的な情報でもログに出力し、デバッグの効率を上げる。
    *   日付パース失敗時には、パースしようとした文字列と試行したパターンをログに残す。
5.  **限界の認識と報告**:
    *   上記努力にもかかわらず、`view_text_website` の制約によりどうしても解析・抽出が不可能なサイトについては、その旨を明確に記録し、今後のための課題として整理する。Seleniumのようなより高度なツールが必要であることを明記する。

このプロジェクトは、各社ウェブサイトの構造に大きく依存するため、継続的なメンテナンスとセレクタ情報の更新が重要となります。ご期待に沿えるよう、粘り強く改善に取り組みます。
